---
title: 最小二乘法为什么使用平方误差
tags:
  - machine learning
category:
  - 学习一个
date: 2018-12-16 02:41:38
---


写这篇博客的动机是因为在知乎上看到了问题[为什么最小二乘法要用最小误差平方和来拟合（为什么不用立方和，或者开方和）](https://www.zhihu.com/question/20792901)？从统计的角度来看，这是使用高斯分布建模噪声+最大似然估计的结果。

<!--more-->
现在假设你有一个数据集 $\{(x_i, y_i)\}^{n}_{i=1}$ , 其中$x_i \in R^d$，  $y_i \in R$。 现在我们希望用一个线性函数$y = w^Tx + b$ 来拟合这组数据。我们知道这组数据未必严格地分布在某条直线或者某个超平面上，也就是说，我们应当假设
$$y_i = w^{T}x_i + \epsilon_i$$
这里$\epsilon_i$是数据中的噪声。接下来我们使用**最大似然估计**来得到参数的估计值（一句话解释最大似然估计：对潜在参数的估计值应当使得观测结果出现的概率最大）。首先写下似然函数的表达式

$$
p(y | w, b, X) = \prod_{i=0}^n p(y_i | w, b, X) \\
               = \prod_{i=0}^n p(\epsilon_i = y_i - (w^Tx_i + b))
$$

现在，**假设$\epsilon_i$独立同分布于$N(0, \sigma^2)$**, 那么
$$
p(\epsilon_i = y_i - (w^Tx_i + b) = \frac{1}{\sqrt{2\pi} \sigma} exp(-\frac{(y_i - (w^Tx_i + b))^2}{\sigma ^ 2})
$$

于是
$$
p(y | w, b, X) = \prod_{i=0}^n \frac{1}{\sqrt{2\pi} \sigma} exp(\frac{(y_i - (w^Tx_i + b))^2}{\sigma ^ 2}) \\
\propto exp(-\sum_{i=1}^n \frac{(y_i - (w^Tx_i + b))^2}{\sigma ^ 2})
$$

显然，最大化似然函数等价于最小化平方误差和
$$
\sum_{i=1}^n (y_i - (w^Tx_i + b))^2
$$
这就得到了我们熟悉的最小二乘法。

类似地，如果我们假设噪声$\epsilon_i$独立同分布于[拉普拉斯分布](https://zh.wikipedia.org/wiki/拉普拉斯分布)， 那么目标函数应为绝对值误差的和，而非平方误差。
